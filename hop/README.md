# Apache Hop - DataLab v2

## ğŸ”„ VisÃ£o Geral

O Apache Hop foi integrado ao DataLab v2 como a principal ferramenta para design visual de ETL/ELT. O Hop Ã© uma evoluÃ§Ã£o do Pentaho Data Integration (Kettle) com arquitetura moderna, suporte nativo a nuvem e integraÃ§Ã£o com tecnologias big data.

## ğŸš€ Acesso e Login

### Interface Web
- **URL**: http://localhost:8081/hop/
- **UsuÃ¡rio**: admin
- **Senha**: supersecretpassword (configurÃ¡vel via variÃ¡vel `HOP_PASSWORD` no .env)

## ğŸ—ï¸ Arquitetura do Apache Hop

### **Conceitos Principais**
- **Pipelines**: Fluxos de dados (antigo "Transformations")
- **Workflows**: OrquestraÃ§Ã£o de pipelines (antigo "Jobs")
- **Projects**: Conjunto de pipelines e workflows organizados
- **Environments**: ConfiguraÃ§Ãµes especÃ­ficas por ambiente (dev, prod, etc.)

## ğŸ”— Conectividade PrÃ©-Configurada

O Apache Hop estÃ¡ configurado com variÃ¡veis de ambiente para conectar-se facilmente aos outros serviÃ§os:

### ğŸ“Š **PostgreSQL**
- **Host**: `${POSTGRES_HOST}` (postgres)
- **Porta**: `${POSTGRES_PORT}` (5432)
- **Database**: `apache_hop` (banco de metadados do Hop)
- **UsuÃ¡rio**: `${POSTGRES_USERNAME}` (admin)
- **Senha**: `${POSTGRES_PASSWORD}`

### ğŸ—„ï¸ **MinIO/S3**
- **Endpoint**: `${S3_ENDPOINT}` (http://minio:9000)
- **Access Key**: `${S3_ACCESS_KEY}`
- **Secret Key**: `${S3_SECRET_KEY}`
- **Bucket Bronze**: `${S3_BRONZE_BUCKET}` (bronze)
- **Bucket Silver**: `${S3_SILVER_BUCKET}` (silver)
- **Bucket Gold**: `${S3_GOLD_BUCKET}` (gold)

### âš¡ **Apache Spark**
- **Master URL**: `${SPARK_MASTER}` (spark://spark-master:7077)
- **Web UI**: `${SPARK_WEB_UI}` (http://spark-master:8080)

### ğŸ”„ **IntegraÃ§Ã£o com Outros ServiÃ§os**
- **Prefect API**: `${PREFECT_API}` (http://prefect-server:4200/api)
- **MLflow**: `${MLFLOW_TRACKING_URI}` (http://mlflow-server:5000)
- **Nessie Catalog**: `${NESSIE_URI}` (http://nessie:19120/api/v2)

## ğŸ“ Estrutura de Projetos

### **Projeto Template: DataLab ETL**
Localizado em `/opt/hop/projects/datalab-etl`, contÃ©m:

```
datalab-etl/
â”œâ”€â”€ metadata/          # ConexÃµes, datasets, etc.
â”œâ”€â”€ pipelines/         # TransformaÃ§Ãµes de dados
â”œâ”€â”€ workflows/         # OrquestraÃ§Ã£o
â”œâ”€â”€ tests/            # Testes unitÃ¡rios
â”œâ”€â”€ datasets/         # Dados de exemplo (CSV)
â”œâ”€â”€ data/             # Dados locais
â””â”€â”€ logs/             # Logs de execuÃ§Ã£o
```

## ğŸ› ï¸ Casos de Uso Principais

### **1. ETL Tradicional**
```
Source Systems â†’ Hop Pipeline â†’ Staging (S3) â†’ Data Warehouse
```

### **2. ELT Moderno**
```
Source Systems â†’ Hop Pipeline â†’ Data Lake (S3) â†’ Spark Processing
```

### **3. Real-time Streaming**
```
Kafka/Queue â†’ Hop Streaming Pipeline â†’ Real-time Analytics
```

### **4. Data Quality**
```
Raw Data â†’ Hop Validation Pipeline â†’ Quality Reports â†’ Clean Data
```

## ğŸ”§ Principais Transforms (Componentes)

### **Input**
- **Table Input**: Consultas SQL em bancos de dados
- **CSV File Input**: Leitura de arquivos CSV
- **Excel Input**: Planilhas Excel
- **JSON Input**: Arquivos e streams JSON
- **S3 File Input**: Objetos do MinIO/S3

### **Transform**
- **Select Values**: SeleÃ§Ã£o e renomeaÃ§Ã£o de campos
- **Filter Rows**: Filtros condicionais
- **Sort Rows**: OrdenaÃ§Ã£o de dados
- **Group By**: AgregaÃ§Ãµes
- **Join Rows**: JunÃ§Ãµes entre streams
- **JavaScript**: Scripts personalizados

### **Output**
- **Table Output**: InserÃ§Ã£o em bancos de dados
- **CSV File Output**: Escrita de arquivos CSV
- **JSON Output**: SaÃ­da em formato JSON
- **S3 File Output**: Upload para MinIO/S3
- **Kafka Producer**: Envio para filas

### **Big Data**
- **Spark Submit**: SubmissÃ£o de jobs Spark
- **Hadoop Copy Files**: OperaÃ§Ãµes HDFS
- **Beam Pipeline**: Apache Beam integration

## ğŸš€ Workflows de Exemplo

### **1. IngestÃ£o DiÃ¡ria de Dados**
```
Start â†’ Check Source â†’ Extract Data â†’ Validate â†’ Transform â†’ Load â†’ Notify
```

### **2. Pipeline de ML**
```
Start â†’ Feature Engineering â†’ Model Training â†’ Model Validation â†’ Deploy â†’ Monitor
```

### **3. Data Quality Check**
```
Start â†’ Data Profiling â†’ Quality Rules â†’ Generate Report â†’ Alert if Issues
```

## ğŸ”„ IntegraÃ§Ã£o com o Ecosystem DataLab

### **Com Spark**
- Hop prepara dados para processamento Spark
- Pode submeter jobs Spark via REST API
- Monitoramento de outputs do Spark

### **Com MLflow**
- Pipeline de feature engineering
- AutomatizaÃ§Ã£o de treinamento de modelos
- Deployamento automatizado baseado em mÃ©tricas

### **Com Prefect**
- Hop workflows podem ser chamados via Prefect
- OrquestraÃ§Ã£o hÃ­brida (Prefect + Hop)
- Monitoramento centralizado

## ğŸ“Š Monitoramento e Logging

### **Logs**
- Logs de execuÃ§Ã£o salvos em `/opt/hop/audit`
- MÃ©tricas de performance por pipeline
- Alertas configurÃ¡veis para falhas

### **MÃ©tricas**
- Throughput de dados processados
- Tempo de execuÃ§Ã£o dos pipelines
- Uso de recursos (CPU, memÃ³ria)

## ğŸ” ConfiguraÃ§Ãµes de SeguranÃ§a

### **Desenvolvimento**
- AutenticaÃ§Ã£o simples (admin/password)
- ConexÃµes HTTP internas
- Dados de teste apenas

### **ProduÃ§Ã£o (RecomendaÃ§Ãµes)**
- LDAP/AD integration
- HTTPS com certificados vÃ¡lidos
- Criptografia de dados sensÃ­veis
- Backup automÃ¡tico de projetos

## ğŸš€ PrÃ³ximos Passos

1. **Acesse a interface**: http://localhost:8080/hop/
2. **Explore o projeto template**: `datalab-etl` em `/opt/hop/projects-templates`
3. **Crie seu primeiro pipeline**: 
   - Conecte ao PostgreSQL
   - Leia dados de uma tabela
   - Transforme os dados
   - Salve no MinIO/S3
4. **Configure conexÃµes**: Use as variÃ¡veis prÃ©-definidas
5. **Teste workflows**: Crie orquestraÃ§Ã£o de mÃºltiplos pipelines

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o Oficial do Apache Hop](https://hop.apache.org/manual/latest/)
- [Apache Hop GitHub](https://github.com/apache/hop)
- [Hop Community](https://hop.apache.org/community/)
- [Tutorials e Examples](https://hop.apache.org/manual/latest/tutorials/)

## ğŸ”§ Troubleshooting

### **Problemas Comuns**

1. **Interface nÃ£o carrega**
   ```bash
   docker logs hop_etl_studio
   ```

2. **ConexÃ£o com PostgreSQL falha**
   - Verifique se o serviÃ§o postgres estÃ¡ rodando
   - Confirme as variÃ¡veis de ambiente

3. **Erro ao acessar MinIO**
   - Verifique se o MinIO estÃ¡ healthy
   - Confirme credenciais S3

4. **Performance lenta**
   - Ajuste `HOP_OPTIONS` para mais memÃ³ria
   - Configure paralelismo nos pipelines

### **Comandos Ãšteis**
```bash
# Restart do Apache Hop
docker-compose restart apache-hop

# Logs detalhados
docker-compose logs -f apache-hop

# Status dos serviÃ§os
docker-compose ps
```
