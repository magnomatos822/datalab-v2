# Exemplo de Pipeline Apache Hop - DataLab v2

## ğŸ“‹ Pipeline: PostgreSQL para MinIO

Este Ã© um exemplo bÃ¡sico de como criar um pipeline no Apache Hop que lÃª dados do PostgreSQL e escreve no MinIO/S3.

### ğŸ¯ Objetivo
Demonstrar um fluxo ETL simples que:
1. Conecta ao PostgreSQL
2. Executa uma query
3. Transforma os dados
4. Salva no bucket do MinIO

### ğŸ› ï¸ Passos para Criar o Pipeline

#### 1. **Acesse o Apache Hop**
- URL: http://localhost:8080/hop/
- URL: http://localhost:8081/hop/
- Login: admin / supersecretpassword

#### 2. **Crie uma Nova ConexÃ£o PostgreSQL**
```
Nome: postgres_datalab
Tipo: PostgreSQL
Host: ${POSTGRES_HOST}
Porta: ${POSTGRES_PORT}
Database: ${POSTGRES_DATABASE}
UsuÃ¡rio: ${POSTGRES_USERNAME}
Senha: ${POSTGRES_PASSWORD}
```

#### 3. **Crie uma ConexÃ£o S3/MinIO**
```
Nome: minio_datalab
Tipo: S3
Endpoint: ${S3_ENDPOINT}
Access Key: ${S3_ACCESS_KEY}
Secret Key: ${S3_SECRET_KEY}
Bucket: ${S3_BUCKET_WAREHOUSE}
```

#### 4. **Estrutura do Pipeline**

```
[Table Input] â†’ [Select Values] â†’ [Filter Rows] â†’ [CSV File Output]
```

##### **Transform 1: Table Input**
- ConexÃ£o: postgres_datalab
- SQL:
```sql
SELECT 
    id,
    name,
    email,
    created_at,
    status
FROM users 
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
```

##### **Transform 2: Select Values**
- Selecionar campos: id, name, email, created_at, status
- Renomear: status â†’ user_status
- Alterar tipo: created_at â†’ String (formato: yyyy-MM-dd)

##### **Transform 3: Filter Rows**
- CondiÃ§Ã£o: user_status <> 'deleted'

##### **Transform 4: CSV File Output**
- Arquivo: s3a://warehouse/users/active_users_${date}.csv
- Separador: ,
- Encoding: UTF-8
- Header: Yes

### ğŸ”„ Workflow de OrquestraÃ§Ã£o

Crie um workflow que:
1. Verifica se hÃ¡ dados novos
2. Executa o pipeline
3. Valida a saÃ­da
4. Envia notificaÃ§Ã£o

```
[Start] â†’ [Check Data] â†’ [Run Pipeline] â†’ [Validate Output] â†’ [Success]
```

### ğŸ“Š Exemplos de Casos de Uso

#### **1. Data Quality Check**
```
[Table Input] â†’ [Data Validator] â†’ [Write Report] â†’ [Alert if Issues]
```

#### **2. Incremental Load**
```
[Get Max Date] â†’ [Extract New Records] â†’ [Transform] â†’ [Load to S3] â†’ [Update Control Table]
```

#### **3. API to Data Lake**
```
[REST Client] â†’ [JSON Input] â†’ [Normalize] â†’ [Partition by Date] â†’ [Parquet Output]
```

### ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

#### **ParalelizaÃ§Ã£o**
- Configure o nÃºmero de cÃ³pias para transforms pesados
- Use "Execute SQL" para controle de transaÃ§Ãµes

#### **Error Handling**
- Adicione steps de tratamento de erro
- Configure hop de erro entre transforms

#### **Logging**
- Ative logging detalhado para debugging
- Configure alerts para falhas

### ğŸ“ˆ Monitoramento

#### **MÃ©tricas Importantes**
- NÃºmero de registros processados
- Tempo de execuÃ§Ã£o
- Taxa de erro
- Throughput (registros/segundo)

#### **Alertas**
- Falha de conexÃ£o com banco/S3
- Pipeline executando por muito tempo
- Qualidade dos dados abaixo do esperado

### ğŸš€ PrÃ³ximos Passos

1. **Teste o pipeline** com dados pequenos primeiro
2. **Configure agendamento** via workflows
3. **Adicione validaÃ§Ãµes** de qualidade de dados
4. **Implemente retry logic** para resiliÃªncia
5. **Configure alertas** para monitoramento

### ğŸ“ Notas

- Use variÃ¡veis de ambiente sempre que possÃ­vel
- Teste em ambiente de desenvolvimento primeiro  
- Documente transformaÃ§Ãµes complexas
- Mantenha pipelines simples e modulares
- Use naming conventions consistentes
