# Apache Spark - DataLab v2

## ‚ö° Vis√£o Geral

O Apache Spark √© o cora√ß√£o do processamento de dados distribu√≠do no DataLab v2. Ele √© configurado para operar em modo cluster com um n√≥ *master* e um ou mais n√≥s *workers*.

-   **Spark Master**: Respons√°vel por coordenar o cluster e distribuir tarefas.
-   **Spark Worker**: Respons√°vel por executar as tarefas de processamento (os "bra√ßos" do cluster).

A integra√ß√£o com o ecossistema √© feita atrav√©s do arquivo `spark-defaults.conf`, que configura o Spark para usar:

-   **MinIO (S3)**: Para ler e escrever dados no Data Lake.
-   **Nessie**: Como cat√°logo de dados para habilitar transa√ß√µes ACID e versionamento (time-travel) sobre as tabelas Iceberg.
-   **Apache Iceberg**: Como o formato de tabela open-source para o Data Lakehouse.

## üîß Configura√ß√£o de Recursos

Voc√™ pode controlar os recursos alocados para cada n√≥ worker diretamente do arquivo `.env`, tornando o ambiente flex√≠vel para diferentes cargas de trabalho.

-   `SPARK_WORKER_CORES`: O n√∫mero de cores de CPU que cada worker pode usar.
-   `SPARK_WORKER_MEMORY`: A quantidade de mem√≥ria RAM alocada para cada worker (ex: `2G`, `512M`).

**Exemplo no `.env`:**

```properties
# Aloca 2 cores e 4GB de RAM para cada worker
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=4G
```

## üöÄ Escalando o Cluster

A arquitetura foi projetada para escalar horizontalmente. Para adicionar mais poder de processamento ao seu cluster, voc√™ pode iniciar m√∫ltiplos containers `spark-worker` com um √∫nico comando.

**Exemplo para iniciar 3 workers:**

```bash
docker-compose up -d --build --scale spark-worker=3
```

Voc√™ pode verificar os workers conectados acessando a UI do Spark Master:
-   **URL**: http://localhost:8082

## üîÑ Integra√ß√£o com dbt

O projeto dbt (`dbt_project/`) √© montado diretamente nos containers do Spark. Isso permite que voc√™ execute modelos dbt que se materializam como tabelas Iceberg no Data Lake, usando o poder de processamento do Spark.

**Para executar o dbt:**

1.  Acesse o container do master:
    ```bash
    docker-compose exec spark-master bash
    ```

2.  Navegue at√© o projeto e execute os comandos dbt:
    ```bash
    cd /opt/bitnami/spark/dbt_project
    dbt run
    dbt test
    ```

## üìö Recursos Adicionais

-   Documenta√ß√£o do Spark sobre configura√ß√£o
-   Integra√ß√£o dbt-spark
-   Projeto Nessie
-   Apache Iceberg